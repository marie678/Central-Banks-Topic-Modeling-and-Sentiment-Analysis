{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project ML for portfolio management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to explore gender differences in central bank communication / speeches using sentiment analysis, inspired by the paper \"Leadership, Gender, and Discourse in Monetary Policy: Analyzing Speech Dynamics in the FOMC\" (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5002334).\n",
    "\n",
    "We will explore gender communication styles in speeches delivered by central bank officials, focusing on whether there are observable differences between male and female central bank leaders.\n",
    "More specifically, using sentiment analysis and NLP techniques, we will analyze whether male and female speakers differ in the topics they address (topic modeling) and the tone of their speeches (sentiment analysis).\n",
    "\n",
    "\n",
    "The speeches would be scraped or downloaded from the BIS website, along with the speaker information. I would need to further define which speeches to use / focus on (based on major policy announcements for instance ?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instructions : 1 : Sharp summary of our results. Should be clear if it works or not.\n",
    "2 : dataset, can be short if reusing. Or longer if webscrapping  summary statistics, code, …\n",
    "3 : why is it an important question, why does it matter ? (central bank data). What is your contribution, what do you bring ? what has been done and what changes with what I’m doing. What is new in what you’re doing.\n",
    "Is the evaluation a backtest or something else, …\n",
    "4 : results : explain what we’re doing and \n",
    "Summary of our test / empirical result. Can be positive or negative. \n",
    "5 : did it work or not, what can be improved / added ?\n",
    "We can also / should !! read a paper and replicate it : google scholar \n",
    "\n",
    "\n",
    "1. Introduction\n",
    "1. Dataset overview\n",
    "1. Analytics and learning strategies\n",
    "1. Empirical resuts: baseline and robustness \n",
    "1. Conclusion\n",
    "\n",
    "if you need to add any package, no problem: add cells in your notebook with \"pip install my_additional_package\" so that I'm aware of what additional packages I need to run your notebook. \n",
    "if you use data that you scrapped online, just provide the code to programmatically scrape the data. More generally, I don't want to receive data.csv files. \n",
    "if you use .py files to tidy your project, just use a %%writefile magic in the notebook -- so that on my side, I can create the same .py files on the fly. I don't want to receive additional .py files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The opening segment encompasses four essential elements:\n",
    "\n",
    "- 1 Contextual Background: What is the larger setting of the study? What makes this area of inquiry compelling? What are the existing gaps or limitations within the current body of research? What are some unanswered yet noteworthy questions?\n",
    "\n",
    "- 2 Project Contributions: What are the specific advancements made by this study, such as in data acquisition, algorithmic development, parameter adjustments, etc.?\n",
    "\n",
    "- 3 Summary of the main empirical results: What is the main statistical statement? is it significant (e.g. statistically or economically)? \n",
    "\n",
    "- 4 Literature and Resource Citations: What are related academic papers? What are the github repositories, expert blogs, or software packages that used in this project? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references : \n",
    "- Leadership, Gender, and Discourse in Monetary Policy: Analyzing Speech Dynamics in the FOMC (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5002334)\n",
    "- Information in Central Bank Sentiment: An Analysis of Fed and ECB Communication (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4797935)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET OVERVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset profile, one should consider:\n",
    "\n",
    "- The origin and composition of data utilized in the study. If the dataset is original, then provide the source code to ensure reproducibility.\n",
    "\n",
    "- The chronological accuracy of the data points, verifying that the dates reflect the actual availability of information.\n",
    "\n",
    "- A detailed analysis of descriptive statistics, with an emphasis on discussing the importance of the chosen graphs or metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central Bank Speeches: You need a corpus of speeches delivered by central bank officials, ideally annotated with speaker information (name, gender, role, country, etc.).\n",
    "Metadata: To analyze gender differences, the dataset must include:\n",
    "Gender of the speaker.\n",
    "Date of the speech.\n",
    "Context (policy announcements, conferences, etc.).\n",
    "\n",
    "scope (e.g., BIS speeches from 2000 onwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20093\n"
     ]
    }
   ],
   "source": [
    "# build functions\n",
    "def get_central_bank_speeches_urls():\n",
    "    '''\n",
    "    '''\n",
    "    url = \"https://www.bis.org/api/document_lists/cbspeeches.json\"\n",
    "    reviews=[]\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        speeches = response.json()\n",
    "        # Get the list of speeches ids ,\n",
    "        for review in speeches['list']:\n",
    "            reviews.append(review)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "    \n",
    "    return(reviews)\n",
    "\n",
    "review_urls = get_central_bank_speeches_urls()\n",
    "print(len(review_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT DONE AT ALL !!!!\n",
    "\n",
    " \n",
    "\n",
    "def clean_directory_path(cache_dir, default_dir=\"data\"):\n",
    "    if cache_dir is None:\n",
    "        cache_dir = Path(os.getcwd()) / default_dir\n",
    "    if isinstance(cache_dir, str):\n",
    "        cache_dir = Path(cache_dir)\n",
    "    if not cache_dir.is_dir():\n",
    "        os.makedirs(cache_dir)\n",
    "    return cache_dir\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_central_banks_speeches(add_url=True, cache_dir=\"data\", force_reload=False, progress_bar=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    filename = clean_directory_path(cache_dir) / \"central_banks_speeches.parquet\"\n",
    "    if (filename.exists()) & (~force_reload):\n",
    "        logger.info(f\"logging from cache file: {filename}\")\n",
    "        speeches = pd.read_parquet(filename)\n",
    "    else:\n",
    "        logger.info(\"loading from external source\")\n",
    "        urls = get_central_bank_speeches_urls()\n",
    "        if progress_bar:\n",
    "            urls_ = tqdm(urls)\n",
    "        else:\n",
    "            urls_ = urls\n",
    "\n",
    "\n",
    "        # get speeches metadata + extract / scrape speech from html page\n",
    "        all_speeches = []\n",
    "        base_url_api = \"https://www.bis.org/api/documents\"\n",
    "        base_url = \"https://www.bis.org\"\n",
    "\n",
    "        counter = 0\n",
    "        switch = None\n",
    "        for link in review_urls : #tqdm(reviews):\n",
    "            counter += 1\n",
    "        #   link = '/review/r010105b'\n",
    "            speech_data = {}\n",
    "            review_url = f'{base_url_api}{link}.json'\n",
    "            speech_url = f'{base_url}{link}.htm'\n",
    "            print(f\"Processing speech: {review_url}\")\n",
    "            try:\n",
    "                # Fetch speech page\n",
    "                review_response = requests.get(review_url)\n",
    "                review_metadata = review_response.json()\n",
    "                speech_data.update(review_metadata)\n",
    "\n",
    "                # Check if 'institutions' exists, skip processing if not\n",
    "                    # only scrape speech if it's the right institution (filtering before scraping)  \n",
    "                if 'institutions' in speech_data: #and speech_data['institutions']==6  and 'publication_start_date' = 2024-12-06\n",
    "                    switch = speech_data['id']\n",
    "                    # print(f\"Skipping speech {review_url} as it does not have the right institution\")       \n",
    "                    try:\n",
    "                        # Scrape speech content\n",
    "                        speech_response = requests.get(speech_url)\n",
    "                        # Parse the HTML content\n",
    "                        soup = BeautifulSoup(speech_response.content, 'html.parser')\n",
    "                        # full speech is ocntained in the 'section' class\n",
    "                        speech_content = soup.find('div', id='cmsContent')\n",
    "                        speech_content_text = speech_content.get_text()\n",
    "                        speech_data['speech_content'] = speech_content_text\n",
    "\n",
    "                    except Exception as e:\n",
    "                            print(f\"Failed to process speech: {speech_url}, error: {e}\")\n",
    "                            continue \n",
    "\n",
    "                    # append speech dict (content + metadata) to all_speeches\n",
    "                    all_speeches.append(speech_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                    print(f\"Failed to fetch data of review {review_url}, status code: {response.status_code}\")\n",
    "                    continue \n",
    "        \n",
    "\n",
    "         speeches = pd.DataFrame(\n",
    "        {\n",
    "            \"release_date\": release_date,\n",
    "            \"last_update\": last_update,\n",
    "            \"text\": text,\n",
    "            \"voting\": voting,\n",
    "            \"release_time\": release_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "        speeches = all_speeches\n",
    "\n",
    "        if add_url:\n",
    "            speeches = speeches.assign(url=urls)\n",
    "        speeches = speeches.sort_index()\n",
    "        logger.info(f\"saving cache file {filename}\")\n",
    "        speeches.to_parquet(filename)\n",
    "    return speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3672091727.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 23\u001b[1;36m\u001b[0m\n\u001b[1;33m    if 'institutions' in speech_data and speech_data['institutions']==6  #and 'publication_start_date' = 2024-12-06\u001b[0m\n\u001b[1;37m                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get speeches metadata + extract / scrape speech from html page\n",
    "all_speeches = []\n",
    "base_url_api = \"https://www.bis.org/api/documents\"\n",
    "base_url = \"https://www.bis.org\"\n",
    "\n",
    "counter = 0\n",
    "switch = None\n",
    "for link in review_urls : #tqdm(reviews):\n",
    "     counter += 1\n",
    "   #   link = '/review/r010105b'\n",
    "     speech_data = {}\n",
    "     review_url = f'{base_url_api}{link}.json'\n",
    "     speech_url = f'{base_url}{link}.htm'\n",
    "     print(f\"Processing speech: {review_url}\")\n",
    "     try:\n",
    "        # Fetch speech page\n",
    "        review_response = requests.get(review_url)\n",
    "        review_metadata = review_response.json()\n",
    "        speech_data.update(review_metadata)\n",
    "\n",
    "        # Check if 'institutions' exists, skip processing if not\n",
    "             # only scrape speech if it's the right institution (filtering before scraping)  \n",
    "        if 'institutions' in speech_data and speech_data['institutions']==6 :  #and 'publication_start_date' = 2024-12-06\n",
    "            switch = speech_data['id']\n",
    "            # print(f\"Skipping speech {review_url} as it does not have the right institution\")       \n",
    "            try:\n",
    "                # Scrape speech content\n",
    "                speech_response = requests.get(speech_url)\n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(speech_response.content, 'html.parser')\n",
    "                # full speech is ocntained in the 'section' class\n",
    "                speech_content = soup.find('div', id='cmsContent')\n",
    "                speech_content_text = speech_content.get_text()\n",
    "                speech_data['speech_content'] = speech_content_text\n",
    "\n",
    "            except Exception as e:\n",
    "                      print(f\"Failed to process speech: {speech_url}, error: {e}\")\n",
    "                      continue \n",
    "\n",
    "              # append speech dict (content + metadata) to all_speeches\n",
    "            all_speeches.append(speech_data)\n",
    "\n",
    "     except Exception as e:\n",
    "            print(f\"Failed to fetch data of review {review_url}, status code: {response.status_code}\")\n",
    "            continue \n",
    "     \n",
    "     # Pause to avoid overloading\n",
    "   #   time.sleep(1)\n",
    "   #   break\n",
    "     \n",
    "\n",
    "# Save results\n",
    "save_path = 'speeches.csv'\n",
    "df = pd.DataFrame(all_speeches)\n",
    "df.to_csv(save_path, index=False)\n",
    "print(f\"Scraped data saved to {save_path}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "219 : 1min7s cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186.11111111111111"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "67*10000/60/60  # --> 3h pour 20000 lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_loughran_mcdonald_dictionary(cache_dir=\"data\", force_reload=False):\n",
    "    \"\"\"\n",
    "    Software Repository for Accounting and Finance by Bill McDonald\n",
    "    https://sraf.nd.edu/loughranmcdonald-master-dictionary/\n",
    "    \"\"\"\n",
    "    filename = (\n",
    "        clean_directory_path(cache_dir)\n",
    "        / \"Loughran-McDonald_MasterDictionary_1993-2021.csv\"\n",
    "    )\n",
    "    if (filename.exists()) & (~force_reload):\n",
    "        logger.info(f\"logging from cache file: {filename}\")\n",
    "    else:\n",
    "        logger.info(\"loading from external source\")\n",
    "        id = \"17CmUZM9hGUdGYjCXcjQLyybjTrcjrhik\"\n",
    "        url = f\"https://docs.google.com/uc?export=download&confirm=t&id={id}\"        \n",
    "        subprocess.run(f\"wget -O '{filename}' '{url}'\", shell=True, capture_output=True)\n",
    "    return pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_10X_summaries(cache_dir=\"data\", force_reload=False):\n",
    "    \"\"\"\n",
    "    Software Repository for Accounting and Finance by Bill McDonald\n",
    "    https://sraf.nd.edu/sec-edgar-data/\n",
    "    \"\"\"\n",
    "    filename = (\n",
    "        clean_directory_path(cache_dir)\n",
    "        / \"Loughran-McDonald_10X_Summaries_1993-2021.csv\"\n",
    "    )\n",
    "    if (filename.is_file()) & (~force_reload):\n",
    "        logger.info(f\"logging from cache directory: {filename}\")\n",
    "    else:\n",
    "        logger.info(\"loading from external source\")\n",
    "        id = \"1CUzLRwQSZ4aUTfPB9EkRtZ48gPwbCOHA\"\n",
    "        url = f\"https://docs.google.com/uc?export=download&confirm=t&id={id}\"\n",
    "        subprocess.run(f\"wget -O '{filename}' '{url}'\", shell=True, capture_output=True)\n",
    "\n",
    "    df = pd.read_csv(filename).assign(\n",
    "        date=lambda x: pd.to_datetime(x.FILING_DATE, format=\"%Y%m%d\")\n",
    "    )\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYTICS AND LEARNING STRATEGY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytics and machine learning methodologies section accounts for:\n",
    "\n",
    "- A detailed explanation of the foundational algorithm.\n",
    "\n",
    "- A description of the data partitioning strategy for training, validation and test.\n",
    "\n",
    "- An overview of the parameter selection and optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMPIRICAL RESULTS : BASELINE AND ROBUSTNESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To effectively convey the empirical findings, separate the baseline results from the additional robustness tests. Within the primary empirical outcomes portion, include:\n",
    "\n",
    "- Key statistical evaluations (for instance, if presenting a backtest – provide a pnl graph alongside the Sharpe ratio).\n",
    "\n",
    "- Insights into what primarily influences the results, such as specific characteristics or assets that significantly impact performance.\n",
    "\n",
    "The robustness of empirical tests section should detail:\n",
    "\n",
    "- Evaluation of the stability of the principal finding against variations in hyperparameters or algorithmic modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the conclusive synthesis should recapitulate the primary findings, consider external elements that may influence the results, and hint at potential directions for further investigative work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
